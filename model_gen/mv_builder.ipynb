{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.losses import Huber, MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (567, 8)\n",
      "All timestamps:    567\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "567"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retrieve_datasets():\n",
    "# Importing Training Set\n",
    "    path=os.path.abspath(os.path.join(\".\", os.pardir))\n",
    "    \n",
    "    eia_df = pd.read_csv(path+'/data/all_eia_stock_sheet_latest.csv',header=0,\n",
    "                                infer_datetime_format=True, delimiter=';',parse_dates=['Date'], index_col=['Date']) \t#Date 2\tAug 27, 1982\n",
    "    \n",
    "    eia_pricing_df = pd.read_csv(path+'/data/eia_pricing_latest.csv',header=0,\n",
    "                                infer_datetime_format=True, delimiter=';',parse_dates=['Date'], index_col=['Date']) \t#Date 2\tAug 27, 1982\n",
    "    #eia_pricing_df = eia_pricing_df.shift(0)\n",
    "    \n",
    "    icsa_df = pd.read_csv(path+'/data/ICSA_current.csv',header=0,\n",
    "                          infer_datetime_format=True, delimiter=',',\n",
    "                          parse_dates=['DATE'] ,index_col=['DATE'])\n",
    "   \n",
    "    #reduce scale to per barrel pricing\n",
    "    eia_df = eia_df.div(10000).round(5)\n",
    "    icsa_df = icsa_df.div(10000).round(5)\n",
    "\n",
    "\n",
    "    #shift back one day to match and pair with s&p dates that line up with EIA (all_barrells_df) dates\n",
    "    icsa_df.index = icsa_df.index + pd.Timedelta(days=-1)\n",
    "    #rename index and shift back 4 periods as unemployement data *should* be indicator later...periods = weeks\n",
    "    icsa_df.index = icsa_df.index.rename('Date')\n",
    "    icsa_df = icsa_df.shift(periods=4)\n",
    "      \n",
    "    return (eia_df, eia_pricing_df, icsa_df)\n",
    "\n",
    "\n",
    "def combine_datasets(datasets):\n",
    "    \"\"\"\n",
    "    We expect an list of dataframes to iterate over and create a final combined df\n",
    "    Expected column key for the join is 'Date'\n",
    "    \"\"\"\n",
    "    df_list = list(range(0, len(datasets)))\n",
    "    dataset_list = list()\n",
    "    for i in df_list:\n",
    "        dataset_list.append(datasets[i])\n",
    "       \n",
    "    combo_df = reduce(lambda  left,right: pd.merge(left,right,on=['Date'],\n",
    "                                                   how='inner'), dataset_list)\n",
    "    combo_df = combo_df.sort_index(ascending=True)\n",
    "\n",
    "\n",
    "    combo_df = combo_df[['WCESTUS1', 'WCESTP21', 'WCRSTUS1', 'WCESTP31', 'WCSSTUS1','WTTSTUS1','ICSA','RCLC1']] #'WCSSTUS1',\n",
    "    combo_df.dropna(inplace=True) #just in case, drop non numbers\n",
    "    \n",
    "    #only use data from 2010 as this is beginning of some of the columns of data in eia barrels data\n",
    "    combo_df = combo_df['2010-01-01':'2021-01-01']\n",
    "\n",
    "    return combo_df\n",
    "\n",
    "\n",
    "datasets = retrieve_datasets()\n",
    "combo_df = combine_datasets(datasets)\n",
    "\n",
    "dataset_train = combo_df\n",
    "\n",
    "#columns to be used in training/prediction\n",
    "cols = ['WCESTUS1', 'WCESTP21', 'WCRSTUS1', 'WCESTP31', 'WCSSTUS1','WTTSTUS1','ICSA','RCLC1']\n",
    "\n",
    "cols = list(cols)\n",
    "\n",
    "# Extract dates (will be used in visualization)\n",
    "datelist_train = list(dataset_train.index)\n",
    "\n",
    "print(f'Training set shape: {dataset_train.shape}')\n",
    "print(f'All timestamps:    {len(datelist_train)}')\n",
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift for training/validation\n",
    "#train_len = int(len(dataset_train*.8)\n",
    "#val_len = len(len(dataset_train)) - train_len\n",
    "\n",
    "#X_val = X_train[train_len:]\n",
    "#y_val = y_train[train_len:]\n",
    "#X_train = X_train[:train_len]\n",
    "#y_train = y_train[:train_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set == (567, 8).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 30.8283,   8.1858, 103.4898, ..., 175.1934,  45.6   ,  74.03  ],\n",
       "       [ 31.0577,   8.0047, 103.7189, ..., 175.3694,  46.9   ,  74.59  ],\n",
       "       [ 31.3662,   7.9276, 104.0274, ..., 175.1277,  50.7   ,  73.91  ],\n",
       "       ...,\n",
       "       [ 48.8721,  14.6251, 112.6911, ..., 199.2034,  75.8   ,  41.7   ],\n",
       "       [ 48.8042,  14.6859, 112.6232, ..., 199.215 ,  75.7   ,  44.8   ],\n",
       "       [ 50.3231,  14.674 , 114.1316, ..., 201.1971,  71.1   ,  45.41  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_train = dataset_train.astype(float)\n",
    "\n",
    "# Using multiple features (predictors)\n",
    "training_set = dataset_train.values\n",
    "\n",
    "print('Shape of training set == {}.'.format(training_set.shape))\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  ndarray\n",
      "shape:  (567, 8)\n",
      "strides:  (8, 4536)\n",
      "itemsize:  8\n",
      "aligned:  True\n",
      "contiguous:  False\n",
      "fortran:  True\n",
      "data pointer: 0x6737290\n",
      "byteorder:  little\n",
      "byteswap:  False\n",
      "type: float64\n"
     ]
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:, 0:1])\n",
    "\n",
    "np.info(training_set_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nX_val = X_train[train_len:]\\ny_val = y_train[train_len:]\\nX_train = X_train[:train_len]\\ny_train = y_train[:train_len]\\n\\n#X_val, y_val = \\n\\nprint(f'X_train shape == {X_train.shape}')\\nprint(f'y_train shape == {y_train.shape}')\\n\\nprint(len(X_train))\\nprint(len(X_val))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a data structure with 90 timestamps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 2   # Number of weeks we want to predict into the future\n",
    "n_past = 4    # Number of past weeks we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(training_set_scaled) - n_future +1):\n",
    "    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1]])\n",
    "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, 0])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "train_len = int(len(X_train)*.8)\n",
    "val_len = len(X_train) - train_len\n",
    "\n",
    "#shift for training/validation\n",
    "\"\"\"\n",
    "X_val = X_train[train_len:]\n",
    "y_val = y_train[train_len:]\n",
    "X_train = X_train[:train_len]\n",
    "y_train = y_train[:train_len]\n",
    "\n",
    "#X_val, y_val = \n",
    "\n",
    "print(f'X_train shape == {X_train.shape}')\n",
    "print(f'y_train shape == {y_train.shape}')\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create a model & Training</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1])))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(units=256, return_sequences=False))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units=1, activation='relu'))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "#model.compile(optimizer = Adam(learning_rate=0.001), loss=Huber())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Start training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 16)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/kbarnes/Development/ML/multivariate-lstm-master/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3418\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-8-cbfa934bab2e>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', \"MAX_EPOCHS = 100\\n#es = EarlyStopping(monitor='val_loss', mode='auto',min_delta=1e-7, patience=10, verbose=0)\\nrlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=0)\\nmcp = ModelCheckpoint(filepath='models/mv_latest_weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True)\\ntb = TensorBoard('logs')\\n#callbacks=[es, rlr, mcp, tb]\\n#history = model.fit(X_train, y_train, shuffle=True, epochs=25, validation_split=0.05, verbose=1, batch_size=32,callbacks=[es, rlr, mcp, tb]) \\n\\n#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,mode='min')\\n\\n\\nearly_stopping = EarlyStopping(monitor='loss', patience=10, mode='auto', min_delta=1e-5)\\n\\nmodel.compile(optimizer = Adam(learning_rate=0.001), loss=Huber())\\nhistory = model.fit(X_train, y_train, shuffle=True, batch_size=16, epochs=MAX_EPOCHS,\\n                    ,callbacks=[early_stopping])\\n#validation_data=(X_val, y_val)\\n\")\n",
      "  File \u001b[1;32m\"/home/kbarnes/Development/ML/multivariate-lstm-master/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2382\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[1;32m\"<decorator-gen-54>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/home/kbarnes/Development/ML/multivariate-lstm-master/venv/lib/python3.8/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/home/kbarnes/Development/ML/multivariate-lstm-master/venv/lib/python3.8/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1277\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/kbarnes/Development/ML/multivariate-lstm-master/venv/lib/python3.8/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m101\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    ,callbacks=[early_stopping])\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MAX_EPOCHS = 100\n",
    "#es = EarlyStopping(monitor='val_loss', mode='auto',min_delta=1e-7, patience=10, verbose=0)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, verbose=0)\n",
    "mcp = ModelCheckpoint(filepath='models/mv_latest_weights.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True)\n",
    "tb = TensorBoard('logs')\n",
    "#callbacks=[es, rlr, mcp, tb]\n",
    "#history = model.fit(X_train, y_train, shuffle=True, epochs=25, validation_split=0.05, verbose=1, batch_size=32,callbacks=[es, rlr, mcp, tb]) \n",
    "\n",
    "#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,mode='min')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, mode='auto', min_delta=1e-5)\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.001), loss=Huber())\n",
    "history = model.fit(X_train, y_train, shuffle=True, batch_size=16, epochs=MAX_EPOCHS,callbacks=[early_stopping])\n",
    "#validation_data=(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of sequence of days for predictions\n",
    "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='7d').tolist()\n",
    "print((datelist_train[-1]))\n",
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_future:\n",
    "    datelist_future_.append(this_timestamp.date())\n",
    "    \n",
    "datelist_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions_future = model.predict(X_train[-n_future:])\n",
    "\n",
    "predictions_train = model.predict(X_train[n_past:])\n",
    "\n",
    "print(np.info(predictions_future))\n",
    "print('------------------')\n",
    "print(np.info(predictions_train))\n",
    "print('------------------')\n",
    "print(predictions_future)\n",
    "print('------------------')\n",
    "print(predictions_future.reshape(2,1))\n",
    "predictions_future = predictions_future.reshape(2,1)\n",
    "predictions_train = predictions_train.reshape(len(predictions_train),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
    "\n",
    "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
    "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['WCESTUS1']).set_index(pd.Series(datelist_future))\n",
    "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['WCESTUS1']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
    "\n",
    "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
    "PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
    "\n",
    "#re-multiply to return to orig sizes\n",
    "dataset_train.WCESTUS1 = dataset_train.WCESTUS1.multiply(10000).round(0)\n",
    "PREDICTIONS_FUTURE = PREDICTIONS_FUTURE.multiply(10000).round(0)\n",
    "PREDICTION_TRAIN['WCESTUS1'] = PREDICTION_TRAIN['WCESTUS1'].multiply(10000).round(0)\n",
    "PREDICTION_TRAIN,PREDICTIONS_FUTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #6. Visualize the Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "# Plot parameters\n",
    "START_DATE_FOR_PLOTTING = '2019-11-01'\n",
    "\n",
    "plt.plot(PREDICTIONS_FUTURE.index, PREDICTIONS_FUTURE['WCESTUS1'], color='r', label='Predicted Stock Price')\n",
    "plt.plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:]['WCESTUS1'], color='orange', label='Training predictions')\n",
    "plt.plot(dataset_train.loc[START_DATE_FOR_PLOTTING:].index, dataset_train.loc[START_DATE_FOR_PLOTTING:]['WCESTUS1'], color='b', label='Actual Stock Price')\n",
    "\n",
    "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.grid(which='major', color='#cccccc', alpha=0.2)\n",
    "\n",
    "plt.legend(shadow=True)\n",
    "plt.title('Predcitions and Acutal Stock Prices', family='Times', fontsize=12)\n",
    "plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
    "plt.ylabel('Stock Price Value', family='Arial', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
