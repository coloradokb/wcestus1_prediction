# -*- coding: utf-8 -*-
"""CNN/LSTM / Univariate - barrels_predictions wcestus1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kIjdM4idBoYWlNIhQ3dH8_0G1nK9znCb
"""
import sys
import tensorflow as tf
import csv
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
	
# univariate cnn lstm example
from numpy import array
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D



print(tf.__version__)

time_step = []
quantity  = []

df = pd.read_csv('data/wcestus1.csv',sep=',')
date_list = df.DATE
barrel_seq= df.drop(columns='DATE')
#print(df.head())
print(date_list[len(date_list)-8:])
print(barrel_seq)

def plot_series(time, series, format="-", start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.grid(True)

#Take a look at the data:
time = np.arange(len(barrel_seq))
#plt.figure(figsize=(10, 6))
#plot_series(time, barrel_seq)

scaler = MinMaxScaler()
scaler.fit(barrel_seq)

barrel_seq = scaler.transform(barrel_seq)


#let's create a training and testing set:
#copy_set   = barrel_seq.copy()
#train_set  = copy_set[:(len(copy_set)-52)]
#test_set   = copy_set[(len(copy_set)-53):-1]
print(len(barrel_seq))
# univariate cnn lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D

# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)

# define input sequence
#raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]
# choose a number of time steps
n_steps = 16
# split into samples
X, y = split_sequence(barrel_seq, n_steps)
# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]
n_features = 1
n_seq = 8
n_steps = 2
X = X.reshape((X.shape[0], n_seq, n_steps, n_features))

#=====================================================
# define model for quick look at learning rate tuning
#=====================================================
def create_model(epochs,n_steps,n_features,verbosity,X,y):
	model = Sequential()
	model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))
	model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
	model.add(TimeDistributed(Flatten()))
	model.add(LSTM(50, activation='tanh'))
	model.add(Dense(1))
	model.compile(optimizer='adam', loss='mse')

	#opt = Adam(lr=0.00001)
	#model.compile(optimizer=opt, loss='mae')
	# revisit for best lr
	#lr_schedule = tf.keras.callbacks.LearningRateScheduler(
	#	lambda epoch: 1e-8 * 10**(epoch / 20))
	history = model.fit(X,y, epochs=epochs,verbose=verbosity)
	#print(history.history['loss'])
	#sys.exit()
	return model

avg = []
def make_prediction(model,size):
	seq_pred_start = len(barrel_seq) - size
	print(seq_pred_start)
	x_input = np.array(barrel_seq[seq_pred_start:])
	#print(scaler.inverse_transform(x_input))
	n_features = 1
	n_seq = int(size/2) #split into half window size
	n_steps = 2
	x_input = x_input.reshape((1, n_seq, n_steps, n_features))
	yhat = pred_model.predict(x_input, verbose=0)
	print(f"STEPS: {n_steps}")
	#print(scaler.inverse_transform(yhat))
	global avg
	avg = np.append(avg,scaler.inverse_transform(yhat))
	return scaler.inverse_transform(yhat)

def write_to_file(out_type,file_loc):
	fp = open

window_sizes = [8,12,16]
for size in window_sizes:
	print(f"Starting: {size}")
	pred_values = []
	epoch_sizes = [250, 500, 1000]
	for epoch in epoch_sizes:
		for x in range(0, 3):
			pred_model = None
			#print(f"IN: {size}")
			n_steps = size
			# split into samples
			X, y = split_sequence(barrel_seq, n_steps)
			n_features = 1
			n_seq = int(size/2)
			#now reshape for conv
			n_steps = 2
			X = X.reshape((X.shape[0], n_seq, n_steps, n_features))

			#x_input = x_input.reshape((1, n_seq, n_steps, n_features))

			pred_model = create_model(epoch,n_steps,n_features,0,X,y)
			pred_values = np.append(pred_values,make_prediction(pred_model,size))
			print(f"LOOP #: {x} during window: {size} and epoch size: {epoch}")
		print(f"Windows={size} & Epoch size= {epoch} / Results:")
		print(pred_values)
		print("Averages:")
		print(np.mean(pred_values))
		pred_values = []

